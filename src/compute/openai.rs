use std::env;

use langchain_rust::llm::openai::OpenAI;
use langchain_rust::llm::OpenAIConfig;

/// Creates an OpenAI langchain client.
///
/// Will check for the following environment variables:
///
/// - `OPENAI_API_BASE`
/// - `OPENAI_API_KEY`
/// - `OPENAI_ORG_ID`
/// - `OPENAI_PROJECT_ID`
pub fn create_openai() -> OpenAI<OpenAIConfig> {
    let mut config = OpenAIConfig::default();

    if let Ok(api_base) = env::var("OPENAI_API_BASE") {
        config = config.with_api_base(api_base);
    }
    if let Ok(api_key) = env::var("OPENAI_API_KEY") {
        config = config.with_api_key(api_key);
    }
    if let Ok(org_id) = env::var("OPENAI_ORG_ID") {
        config = config.with_org_id(org_id);
    }
    if let Ok(project_id) = env::var("OPENAI_PROJECT_ID") {
        config = config.with_project_id(project_id);
    }

    OpenAI::new(config)
}

#[cfg(test)]
mod tests {
    use super::*;
    use langchain_rust::language_models::llm::LLM;

    #[tokio::test]
    #[ignore] // cargo test --package dkn-compute --lib --all-features -- compute::openai::tests::test_openai --exact --show-output --ignored
    async fn test_openai() {
        let value = "FOOBARFOOBAR"; // use with your own key, with caution
        env::set_var("OPENAI_API_KEY", value);

        let openai = create_openai();

        let prompt = "Once upon a time, in a land far away, there was a dragon.";
        let response = openai
            .invoke(prompt)
            .await
            .expect("Should generate response");
        println!("{}", response);
    }
}
